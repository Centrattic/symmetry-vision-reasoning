{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SimpleVLM\n",
    "from data import generate_clock_image, text_to_indices, generate_dataset, ClockVLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pth_path, device, vocab_size, embed_dim, text_hidden_dim, num_classes):\n",
    "    model = SimpleVLM(vocab_size, embed_dim, text_hidden_dim, num_classes)\n",
    "    model.load_state_dict(torch.load(pth_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intermediate_representation(model, sample_image, sample_text, device):\n",
    "    activations = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        # Save output from the img_encoder (before fc)\n",
    "        activations[\"img_encoder\"] = output.detach()\n",
    "    hook_handle = model.img_encoder.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass (add batch dimension)\n",
    "    sample_image = sample_image.unsqueeze(0).to(device)\n",
    "    sample_text = sample_text.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(sample_image, sample_text)\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Return the captured activation.\n",
    "    # Shape is (1, 64, 1, 1)\n",
    "    return activations[\"img_encoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, sample_image, sample_text, device):\n",
    "    # Make sure sample_image has grad enabled.\n",
    "    sample_image = sample_image.unsqueeze(0).to(device)\n",
    "    sample_image.requires_grad_()\n",
    "    sample_text = sample_text.unsqueeze(0).to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    output = model(sample_image, sample_text)\n",
    "    # Choose the highest scoring class.\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    # Take the logit for the predicted class.\n",
    "    score = output[0, pred_class]\n",
    "    score.backward()\n",
    "    \n",
    "    # Saliency: take the absolute value of gradients.\n",
    "    saliency, _ = torch.max(sample_image.grad.data.abs(), dim=1)\n",
    "    # saliency shape: (1, H, W)\n",
    "    saliency = saliency.squeeze().cpu().numpy()\n",
    "    return saliency, pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(model, dataset, device, save_path=\"tsne_plot.png\"):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for img, text, label in loader:\n",
    "            img = img.to(device)\n",
    "            text = text.unsqueeze(0).repeat(img.size(0), 1).to(device) if text.dim() == 1 else text.to(device)\n",
    "            # Extract features from the image branch BEFORE the fully connected layer.\n",
    "            feat = model.img_encoder(img)\n",
    "            feat = feat.view(feat.size(0), -1)  # shape: (B, 64)\n",
    "            features.append(feat.cpu())\n",
    "            labels.extend(label.tolist())\n",
    "    \n",
    "    features = torch.cat(features, dim=0).numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    scatter = plt.scatter(features_2d[:,0], features_2d[:,1], c=labels, cmap='tab10')\n",
    "    plt.colorbar(scatter, label='Class Label')\n",
    "    plt.title(\"t-SNE of Vision Encoder Features\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"t-SNE visualization saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample clock image saved as sample_clock.png\n",
      "Extracted intermediate representation (vision encoder output) shape: torch.Size([1, 64, 1, 1])\n",
      "Intermediate representation plot saved as intermediate_representation.png\n",
      "Predicted class for sample image: 16\n",
      "Saliency map saved as saliency_map.png\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 79\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaliency map saved as saliency_map.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# 3. t-SNE Visualization of the Entire Dataset's Image Features\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# For t-SNE, generate the full dataset.\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m images, labels, time_strings \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m(n\u001b[38;5;241m=\u001b[39mn, size\u001b[38;5;241m=\u001b[39mimage_size)\n\u001b[0;32m     80\u001b[0m dataset \u001b[38;5;241m=\u001b[39m ClockVLMDataset(images, labels, prompt, vocab, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     81\u001b[0m visualize_tsne(model, dataset, device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuration parameters (should match training settings)\n",
    "n = 5\n",
    "image_size = 128\n",
    "num_classes = n * n\n",
    "\n",
    "# Text prompt and vocabulary (must match training)\n",
    "prompt = \"Tell me the time on the clock\"\n",
    "vocab = {\"tell\": 0, \"me\": 1, \"the\": 2, \"time\": 3, \"on\": 4, \"clock\": 5}\n",
    "\n",
    "# Define image transformation (must match training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# Device configuration.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model hyperparameters for text branch.\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 32\n",
    "text_hidden_dim = 128\n",
    "\n",
    "# Load the saved model.\n",
    "model_path = \"clock_vlm_model_5_128_100_4_0.0005.pth\"\n",
    "model = load_model(model_path, device, vocab_size, embed_dim, text_hidden_dim, num_classes)\n",
    "\n",
    "# Generate one sample image for interpretability.\n",
    "sample_img_np, time_str = generate_clock_image(hour=3, minute=2, n=n, size=image_size)\n",
    "# Save sample image for reference.\n",
    "Image.fromarray(sample_img_np).save(\"sample_clock.png\")\n",
    "print(\"Sample clock image saved as sample_clock.png\")\n",
    "\n",
    "# Apply transform.\n",
    "sample_img = transform(Image.fromarray(sample_img_np))\n",
    "# Prepare text indices tensor.\n",
    "sample_text_indices = torch.tensor(text_to_indices(prompt, vocab), dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Intermediate Representation\n",
    "# ---------------------------\n",
    "rep = extract_intermediate_representation(model, sample_img, sample_text_indices, device)\n",
    "# rep has shape (1, 64, 1, 1) so we squeeze spatial dims.\n",
    "rep_vector = rep.view(-1).cpu().numpy()\n",
    "print(\"Extracted intermediate representation (vision encoder output) shape:\", rep.shape)\n",
    "# For visualization, we can plot a bar graph of the 64 features.\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(np.arange(len(rep_vector)), rep_vector)\n",
    "plt.title(\"Intermediate Features from Vision Encoder (flattened)\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.savefig(\"intermediate_representation.png\")\n",
    "plt.close()\n",
    "print(\"Intermediate representation plot saved as intermediate_representation.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Saliency Map Computation\n",
    "# ---------------------------\n",
    "saliency, pred_class = compute_saliency_map(model, sample_img, sample_text_indices, device)\n",
    "print(\"Predicted class for sample image:\", pred_class)\n",
    "# Plot and overlay the saliency map on the original image.\n",
    "plt.figure(figsize=(6,6))\n",
    "# Show original image.\n",
    "orig_img = sample_img.cpu().permute(1,2,0).numpy()\n",
    "# Un-normalize the image (assuming normalization: (x-0.5)/0.5)\n",
    "orig_img = (orig_img * 0.5) + 0.5\n",
    "plt.imshow(orig_img)\n",
    "plt.imshow(saliency, cmap='jet', alpha=0.5)\n",
    "plt.title(\"Saliency Map Overlay\")\n",
    "plt.axis('off')\n",
    "plt.savefig(\"saliency_map.png\")\n",
    "plt.close()\n",
    "print(\"Saliency map saved as saliency_map.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. t-SNE Visualization of the Entire Dataset's Image Features\n",
    "# ---------------------------\n",
    "# For t-SNE, generate the full dataset.\n",
    "images, labels, time_strings = generate_dataset(n=n, size=image_size)\n",
    "dataset = ClockVLMDataset(images, labels, prompt, vocab, transform=transform)\n",
    "visualize_tsne(model, dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
