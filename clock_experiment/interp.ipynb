{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u admin\\AppData\\Local\\Temp\\ipykernel_15248\\3146889506.py:5: MatplotlibDeprecationWarning: Auto-close()ing of figures upon backend switching is deprecated since 3.8 and will be removed in 3.10.  To suppress this warning, explicitly call plt.close('all') first.\n",
      "  matplotlib.use(\"TkAgg\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SimpleVLMSeq2Seq\n",
    "from data import generate_clock_image, text_to_indices, generate_dataset, ClockVLMDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pth_path, device, vocab_size, embed_dim, text_hidden_dim, num_classes):\n",
    "    model = SimpleVLMSeq2Seq(vocab_size, embed_dim, text_hidden_dim, num_classes)\n",
    "    model.load_state_dict(torch.load(pth_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_intermediate_representation(model, sample_image, sample_text, device):\n",
    "    activations = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        # Save output from the img_encoder (before fc)\n",
    "        activations[\"img_encoder\"] = output.detach()\n",
    "    hook_handle = model.img_encoder.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass (add batch dimension)\n",
    "    sample_image = sample_image.unsqueeze(0).to(device)\n",
    "    sample_text = sample_text.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(sample_image, sample_text)\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Return the captured activation.\n",
    "    # Shape is (1, 64, 1, 1)\n",
    "    return activations[\"img_encoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, sample_image, sample_text, device):\n",
    "    # Make sure sample_image has grad enabled.\n",
    "    sample_image = sample_image.unsqueeze(0).to(device)\n",
    "    sample_image.requires_grad_()\n",
    "    sample_text = sample_text.unsqueeze(0).to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    output = model(sample_image, sample_text)\n",
    "    # Choose the highest scoring class.\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "    # Take the logit for the predicted class.\n",
    "    score = output[0, pred_class]\n",
    "    score.backward()\n",
    "    \n",
    "    # Saliency: take the absolute value of gradients.\n",
    "    saliency, _ = torch.max(sample_image.grad.data.abs(), dim=1)\n",
    "    # saliency shape: (1, H, W)\n",
    "    saliency = saliency.squeeze().cpu().numpy()\n",
    "    return saliency, pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(model, dataset, device, save_path=\"tsne_plot.png\"):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for img, text, label in loader:\n",
    "            img = img.to(device)\n",
    "            text = text.unsqueeze(0).repeat(img.size(0), 1).to(device) if text.dim() == 1 else text.to(device)\n",
    "            # Extract features from the image branch BEFORE the fully connected layer.\n",
    "            feat = model.img_encoder(img)\n",
    "            feat = feat.view(feat.size(0), -1)  # shape: (B, 64)\n",
    "            features.append(feat.cpu())\n",
    "            labels.extend(label.tolist())\n",
    "    \n",
    "    features = torch.cat(features, dim=0).numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=5,random_state=42)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "    \n",
    "    n = 5  # Set this to the discrete clock positions used in your dataset\n",
    "    hour_labels = np.array(labels) // n  # Integer division to get the hour\n",
    "\n",
    "    # Create the scatter plot using the hour_labels for coloring.\n",
    "    plt.figure(figsize=(6,6))\n",
    "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=hour_labels, cmap='tab10')\n",
    "    plt.colorbar(scatter, label='Hour')\n",
    "    plt.title(\"t-SNE of Vision Encoder Features (Colored by Hour)\")\n",
    "    plt.show()\n",
    "    # plt.savefig(\"tsne_plot.png\")\n",
    "    plt.close()\n",
    "    # print(\"t-SNE plot saved as tsne_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted intermediate representation (vision encoder output) shape: torch.Size([1, 64, 1, 1])\n",
      "Predicted class for sample image: 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaliency Map Overlay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# plt.savefig(\"saliency_map.png\")\u001b[39;00m\n\u001b[0;32m     75\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Conda-Julia\\3\\Lib\\site-packages\\matplotlib\\pyplot.py:612\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    611\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Conda-Julia\\3\\Lib\\site-packages\\matplotlib\\backend_bases.py:3553\u001b[0m, in \u001b[0;36m_Backend.show\u001b[1;34m(cls, block)\u001b[0m\n\u001b[0;32m   3551\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m ipython_pylab \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive()\n\u001b[0;32m   3552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m-> 3553\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Conda-Julia\\3\\Lib\\site-packages\\matplotlib\\backends\\_backend_tk.py:520\u001b[0m, in \u001b[0;36mFigureManagerTk.start_main_loop\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    518\u001b[0m manager_class\u001b[38;5;241m.\u001b[39m_owns_mainloop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m     \u001b[43mfirst_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     manager_class\u001b[38;5;241m.\u001b[39m_owns_mainloop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Conda-Julia\\3\\Lib\\tkinter\\__init__.py:1505\u001b[0m, in \u001b[0;36mMisc.mainloop\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmainloop\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1505\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmainloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configuration parameters (should match training settings)\n",
    "n = 5\n",
    "image_size = 128\n",
    "num_classes = n * n\n",
    "\n",
    "# Text prompt and vocabulary (must match training)\n",
    "prompt = \"Tell me the time on the clock\"\n",
    "vocab = {\"tell\": 0, \"me\": 1, \"the\": 2, \"time\": 3, \"on\": 4, \"clock\": 5}\n",
    "\n",
    "# Define image transformation (must match training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "# Device configuration.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model hyperparameters for text branch.\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 32\n",
    "text_hidden_dim = 128\n",
    "\n",
    "# Load the saved model.\n",
    "model_path = \"clock_vlm_model_5_128_100_4_0.0005.pth\"\n",
    "model = load_model(model_path, device, vocab_size, embed_dim, text_hidden_dim, num_classes)\n",
    "\n",
    "# Generate one sample image for interpretability.\n",
    "sample_img_np, time_str = generate_clock_image(hour=3, minute=2, n=n, size=image_size)\n",
    "# Save sample image for reference.\n",
    "plt.imshow(sample_img_np)\n",
    "# Image.fromarray(sample_img_np).save(\"sample_clock.png\")\n",
    "# print(\"Sample clock image saved as sample_clock.png\")\n",
    "\n",
    "# Apply transform.\n",
    "sample_img = transform(Image.fromarray(sample_img_np))\n",
    "# Prepare text indices tensor.\n",
    "sample_text_indices = torch.tensor(text_to_indices(prompt, vocab), dtype=torch.long)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Intermediate Representation\n",
    "# ---------------------------\n",
    "rep = extract_intermediate_representation(model, sample_img, sample_text_indices, device)\n",
    "# rep has shape (1, 64, 1, 1) so we squeeze spatial dims.\n",
    "rep_vector = rep.view(-1).cpu().numpy()\n",
    "print(\"Extracted intermediate representation (vision encoder output) shape:\", rep.shape)\n",
    "# For visualization, we can plot a bar graph of the 64 features.\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(np.arange(len(rep_vector)), rep_vector)\n",
    "plt.title(\"Intermediate Features from Vision Encoder (flattened)\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Activation\")\n",
    "plt.show()\n",
    "# plt.savefig(\"intermediate_representation.png\")\n",
    "plt.close()\n",
    "# print(\"Intermediate representation plot saved as intermediate_representation.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Saliency Map Computation\n",
    "# ---------------------------\n",
    "saliency, pred_class = compute_saliency_map(model, sample_img, sample_text_indices, device)\n",
    "print(\"Predicted class for sample image:\", pred_class)\n",
    "# Plot and overlay the saliency map on the original image.\n",
    "plt.figure(figsize=(6,6))\n",
    "# Show original image.\n",
    "orig_img = sample_img.cpu().permute(1,2,0).numpy()\n",
    "# Un-normalize the image (assuming normalization: (x-0.5)/0.5)\n",
    "orig_img = (orig_img * 0.5) + 0.5\n",
    "plt.imshow(orig_img)\n",
    "plt.imshow(saliency, cmap='jet', alpha=0.5)\n",
    "plt.title(\"Saliency Map Overlay\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# plt.savefig(\"saliency_map.png\")\n",
    "plt.close()\n",
    "# print(\"Saliency map saved as saliency_map.png\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. t-SNE Visualization of the Entire Dataset's Image Features\n",
    "# ---------------------------\n",
    "# For t-SNE, generate the full dataset.\n",
    "images, labels, time_strings = generate_dataset(n=n, size=image_size)\n",
    "dataset = ClockVLMDataset(images, labels, prompt, vocab, transform=transform)\n",
    "visualize_tsne(model, dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "images, labels, time_strings = generate_dataset(n=n, size=image_size)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
