{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from captum.attr import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.OpenGVLab.InternVL2-1B.0d75ccd166b1d0b79446ae6c5d1a4a667f1e6187.configuration_internvl_chat.InternVLChatConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Conda-Julia\\3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:574\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.OpenGVLab.InternVL2-1B.0d75ccd166b1d0b79446ae6c5d1a4a667f1e6187.configuration_internvl_chat.InternVLChatConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model name and load components\n",
    "model_name = \"OpenGVLab/InternVL2-1B\"  # Model repository URL on Hugging Face\n",
    "# The following components assume that InternVL2-1B supports a processor (to jointly handle images and text)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all images (jpg, jpeg, png) from the specified folder.\n",
    "    Returns a list of tuples: (filename, PIL.Image).\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.JPEG')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                images.append((filename, img))\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load image {img_path}: {e}\")\n",
    "    return images\n",
    "\n",
    "def apply_transformations(image):\n",
    "    \"\"\"\n",
    "    Given a PIL Image, apply several symmetry transformations.\n",
    "    Returns a dictionary mapping transformation names to transformed images.\n",
    "    \"\"\"\n",
    "    transformations = {\n",
    "        \"original\": image,\n",
    "        \"rotate_90\": image.rotate(90, expand=True),\n",
    "        \"rotate_180\": image.rotate(180, expand=True),\n",
    "        \"rotate_270\": image.rotate(270, expand=True),\n",
    "        \"flip_horizontal\": image.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "        \"flip_vertical\": image.transpose(Image.FLIP_TOP_BOTTOM),\n",
    "        # Add more custom transformations if needed.\n",
    "    }\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image, prompt=\"What is in the image?\"):\n",
    "    \"\"\"\n",
    "    Given a PIL Image and a text prompt, processes the image using the model's processor,\n",
    "    and generates a textual response from the model.\n",
    "    \"\"\"\n",
    "    # The processor handles both image and text (if supported by the model)\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_saliency(image, prompt=\"What is in the image?\", target_token_position=0, baseline=None):\n",
    "    \"\"\"\n",
    "    Computes a saliency map (pixel attributions) for a chosen output token of the generated answer,\n",
    "    using Captum's Integrated Gradients.\n",
    "    \n",
    "    Parameters:\n",
    "      image: PIL Image.\n",
    "      prompt: Textual prompt provided to the model.\n",
    "      target_token_position: The token position in the output whose logit is used for attribution.\n",
    "      baseline: Baseline tensor for integrated gradients (if None, uses a zero tensor).\n",
    "      \n",
    "    Returns:\n",
    "      attributions: Tensor of attributions (same shape as input image tensor).\n",
    "      delta: Convergence delta from IntegratedGradients.\n",
    "      \n",
    "    Note: This example assumes that the model's processor produces a tensor called 'pixel_values'.\n",
    "    \"\"\"\n",
    "    # Process the image and prompt to obtain model inputs.\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    # Extract the pixel tensor (shape: [1, C, H, W])\n",
    "    image_tensor = inputs[\"pixel_values\"]\n",
    "    \n",
    "    # Ensure gradients are tracked\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    # Define a forward function that takes an image tensor and returns the chosen token logit.\n",
    "    def forward_func(img_tensor):\n",
    "        new_inputs = inputs.copy()\n",
    "        new_inputs[\"pixel_values\"] = img_tensor\n",
    "        # Run the forward pass; here we use the logits output.\n",
    "        outputs = model(**new_inputs)\n",
    "        logits = outputs.logits  # shape: [batch, seq_len, vocab_size]\n",
    "        # For simplicity, we choose the maximum logit value at the target token position.\n",
    "        # (In practice, you might choose the logit for a specific token id.)\n",
    "        target_logit = logits[0, target_token_position, :].max()\n",
    "        return target_logit\n",
    "\n",
    "    # Define a baseline (a black image). If provided, use it; otherwise, use zeros.\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(image_tensor)\n",
    "    \n",
    "    # Initialize Integrated Gradients and compute attributions.\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "    attributions, delta = ig.attribute(image_tensor, baseline, return_convergence_delta=True)\n",
    "    return attributions, delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cross_attention(image, prompt=\"What is in the image?\"):\n",
    "    \"\"\"\n",
    "    Registers forward hooks on cross-attention modules in the modelâ€™s decoder to extract the attention weights.\n",
    "    Returns a list of attention outputs from all layers that have cross-attention.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "    cross_attn_weights = []  # List to store attention outputs\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # For this example, we append the output directly.\n",
    "        # (Depending on the model, you might need to extract specific tensors,\n",
    "        # e.g., the raw attention weights.)\n",
    "        cross_attn_weights.append(output)\n",
    "\n",
    "    hooks = []\n",
    "    # Register hooks on modules whose names indicate cross-attention.\n",
    "    for name, module in model.named_modules():\n",
    "        if \"cross_attn\" in name or \"crossattention\" in name:\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Run a forward pass to collect attention weights.\n",
    "    _ = model(**inputs)\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "        \n",
    "    return cross_attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(folder_path, prompt=\"What is in the image?\"):\n",
    "    \"\"\"\n",
    "    For each image in a folder, applies a set of symmetry transformations, classifies\n",
    "    each transformed image using InternVL2-1B, computes a saliency map on the transformed images,\n",
    "    and extracts cross-attention weights for the original image.\n",
    "    \"\"\"\n",
    "    images = load_images_from_folder(folder_path)\n",
    "    all_results = {}\n",
    "\n",
    "    for filename, img in images:\n",
    "        print(f\"\\n=== Processing Image: {filename} ===\")\n",
    "        transformation_dict = apply_transformations(img)\n",
    "        img_results = {}\n",
    "        \n",
    "        for trans_name, timg in transformation_dict.items():\n",
    "            print(f\"\\nTransformation: {trans_name}\")\n",
    "            # Classification (text generation) from the model:\n",
    "            classification = classify_image(timg, prompt)\n",
    "            print(f\"Classification: {classification}\")\n",
    "            \n",
    "            # Compute and visualize the saliency map (Integrated Gradients)\n",
    "            attributions, delta = compute_image_saliency(timg, prompt, target_token_position=0)\n",
    "            # Average the attributions across color channels to create a 2D saliency map.\n",
    "            attr_np = attributions.squeeze().detach().cpu().numpy()  # shape: [C, H, W]\n",
    "            if attr_np.ndim == 3:\n",
    "                attr_np = np.mean(attr_np, axis=0)  # shape: [H, W]\n",
    "            \n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.imshow(attr_np, cmap=\"hot\", interpolation=\"nearest\")\n",
    "            plt.title(f\"Saliency Map - {trans_name}\")\n",
    "            plt.colorbar()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Save results for this transformation\n",
    "            img_results[trans_name] = {\n",
    "                \"classification\": classification,\n",
    "                \"saliency_delta\": delta.item() if isinstance(delta, torch.Tensor) else delta\n",
    "            }\n",
    "        all_results[filename] = img_results\n",
    "        \n",
    "        # (Optional) Extract cross-attention weights for the original image\n",
    "        ca_weights = extract_cross_attention(img, prompt)\n",
    "        print(\"\\nCross-attention weights (shapes) from decoder layers:\")\n",
    "        for i, weight in enumerate(ca_weights):\n",
    "            # If weight is a tensor, print its shape; otherwise, show type info.\n",
    "            if torch.is_tensor(weight):\n",
    "                print(f\" Layer {i}: shape {tuple(weight.shape)}\")\n",
    "            else:\n",
    "                print(f\" Layer {i}: type {type(weight)}\")\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Change this to your local folder containing images.\n",
    "    folder_path = \"images\"  \n",
    "    results = run_experiment(folder_path, prompt=\"What is in the image?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
