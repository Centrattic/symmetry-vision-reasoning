# Goal

The idea is to attempt to solve ARC using a vision-based approach. We'll first bench vision models for their vision reasoning abilities on ARC, and then attempt to train a visual "reasoning" model that is able to reason on the task somehow visually, using spatial arrangements in some useful way that we've encoded. 

One other idea is to try to work on in-context learning of visual representations! Maybe just making the embedding dimension really big and training a bit will make the model good at this.

Relevant papers:

https://arxiv.org/abs/2407.04973
https://ieeexplore.ieee.org/abstract/document/10650020
https://openaccess.thecvf.com/content/CVPR2024/html/Ganz_Question_Aware_Vision_Transformer_for_Multimodal_Reasoning_CVPR_2024_paper.html
https://arxiv.org/abs/2403.11401
https://ojs.aaai.org/index.php/AAAI/article/view/27888
https://arxiv.org/abs/2501.00070
https://arxiv.org/abs/2402.11574 (Super relevant! In-context learning of visual reps in VLMs)




